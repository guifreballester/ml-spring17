plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
points(test, col=nicecolors[as.numeric(predicted)], pch=".")
contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size),
levels=c(1,2), add=TRUE, drawlabels=FALSE)
# add training points, for reference
points(train, col=nicecolors[t+1], pch=16)
title(paste(myk,"-NN classification",sep=""))
}
wine.data <- subset (wine, select = -Wine.type)
knn.preds <- rep(NA, nrow(wine.data))
for (i in 1:nrow(wine.data))
{
knn.preds[i] <- knn (wine.data[-i,], wine.data[i,], wine$Wine.type[-i], k = 3)
}
(tab <- table(Truth=wine$Wine.type, Preds=knn.preds))
wine.data <- subset (wine, select = -Wine.type)
wine.fl <- "http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine <- read.csv(wine.fl,header = F)
# Names of the variables
wine.names=c("Alcohol", "Malic acid", "Ash", "Alcalinity of ash", "Magnesium",
"Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins",
"Color intensity", "Hue", "OD280/OD315 of diluted wines", "Proline")
colnames(wine)[2:14]=wine.names
colnames(wine)[1]="Class"
wine <- read.table("wine.data", sep=",", dec=".", header=FALSE)
dim(wine)
colnames(wine) <- c('Wine.type','Alcohol','Malic.acid','Ash','Alcalinity.of.ash','Magnesium','Total.phenols','Flavanoids','Nonflavanoid.phenols','Proanthocyanins','Color.intensity','Hue','OD280.OD315','Proline')
wine$Wine.type <- factor(wine$Wine.type, labels=c("Cultivar.1","Cultivar.2","Cultivar.3"))
summary(wine)
plot(subset(wine,select=-Wine.type),col=unclass(wine$Wine.type))
(lda.model <- lda (Wine.type ~ ., data = wine))
wine.data <- subset (wine, select = -Wine.type)
knn.preds <- rep(NA, nrow(wine.data))
for (i in 1:nrow(wine.data))
{
knn.preds[i] <- knn (wine.data[-i,], wine.data[i,], wine$Wine.type[-i], k = 3)
}
(tab <- table(Truth=wine$Wine.type, Preds=knn.preds))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
myknn.cv <- knn.cv (wine.data, wine$Wine.type, k = 3)
(tab <- table(Truth=wine$Wine.type, Preds=myknn.cv))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
## Let's loop over k using a function
set.seed (6046)
N <- nrow(wine)
neighbours <- 1:sqrt(N)
loop.k <- function (mydata, mytargets, myneighbours)
{
errors <- matrix (nrow=length(myneighbours), ncol=2)
colnames(errors) <- c("k","LOOCV error")
for (k in myneighbours)
{
print(k)
myknn.cv <- knn.cv (mydata, mytargets, k = myneighbours[k])
# fill in number of neighbours and LOOCV error
errors[k, "k"] <- myneighbours[k]
tab <- table(Truth=mytargets, Preds=myknn.cv)
errors[k, "LOOCV error"] <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
}
errors
}
par(mfrow=c(1,1))
plot(loop.k (wine.data, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
plot(loop.k (scale(wine.data), wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
lda.model <- lda (Wine.type ~ ., data = wine)
loadings <- as.matrix(wine.data) %*% as.matrix(lda.model$scaling)
## Let's repeat the loop over k
set.seed (6046)
plot(loop.k (loadings, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
library (e1071)
data (HouseVotes84, package="mlbench")
colnames(HouseVotes84) <-
c("Class","handicapped.infants","water.project.sharing","budget.resolution",
"physician.fee.freeze","el.salvador.aid","religious.groups.in.schools",
"anti.satellite.ban", "aid.to.nicaraguan.contras","mx.missile",
"immigration","synfuels.cutback","education.spending","superfund","crime",
"duty.free.exports","export.South.Africa")
summary(HouseVotes84)
set.seed(1111)
N <- nrow(HouseVotes84)
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
model <- naiveBayes(Class ~ ., data = HouseVotes84[learn,])
# we get all the probabilities (so 'eager' mode)
model
# predict the outcome of the first 10 Congressmen
predict(model, HouseVotes84[1:10,-1])
# same but displaying posterior probabilities
predict(model, HouseVotes84[1:10,-1], type = "raw")
# compute now the apparent error
pred <- predict(model, HouseVotes84[learn,-1])
# form and display confusion matrix & overall error
(tab <- table(Truth=HouseVotes84[learn,]$Class, Preds=pred) )
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
# compute the test (prediction) error
pred <- predict(model, newdata=HouseVotes84[-learn,-1])
# form and display confusion matrix & overall error
(tab <- table(Truth=HouseVotes84[-learn,]$Class, Preds=pred) )
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
model <- naiveBayes(Class ~ ., data = HouseVotes84[learn,], laplace = 1)
(Sigma <- matrix(c(4,-3,-3,9),2,2))
(M <- t(chol(Sigma)))
M %*% t(M)
mu <- c(2,-2)
N <- 200
Z <- matrix(rnorm(2*N),2,N) # 2 rows, N columns
X <- t(M %*% Z + mu)
# This is our data (the axes denote the center 'mu'):
#
par(mfrow=c(1,1))
plot(X, xlab="X1", ylab="X2")
abline(v=mean(X[,1]),lty=2)
abline(h=mean(X[,2]),lty=2)
(S <- cov(X))
apply(X,2,mean)
library(MASS)
gauss2 <- mvrnorm (10000, mu = mu, Sigma = Sigma)
# now we can do a kernel density estimate (not seen in class), you can guess what it does:
#
gauss2.kde <- kde2d(gauss2[,1], gauss2[,2], n = 50)
gauss2.kde
# fancy contour
image(gauss2.kde)
contour(gauss2.kde, add = T)
?kde2d
dim(gauss2.kde)
# static 3D plot
persp(gauss2.kde, phi = 3, theta = 10)
#### which can be compared with the theoretical plot for this MVGD
library(ellipse) # you have to install it
install.packages("ellipse")
#### which can be compared with the theoretical plot for this MVGD
library(ellipse) # you have to install it
plot(0,0, type="n", xlim=range(gauss2[,1]), ylim=range(gauss2[,2]), xlab="X1", ylab="X2")
for (l in seq(0.1,0.9, length.out=7))
lines(ellipse(Sigma, centre=mu, level=l), col='blue', lwd=2)
points(mu[1],mu[2], pch=16,col='blue')
?seq
N <- 1000
(Sigma <- matrix(data=c(1,0.3,0.3,1),nrow=2,ncol=2,byrow=TRUE))
eigen (Sigma, only.values=TRUE)$values
mean.1 <- matrix(c(2,0),nrow=2,ncol=1)
X.red <- mvrnorm(N,mu=mean.1,Sigma=Sigma)
mean.2 <- -mean.1
X.green <- mvrnorm(N,mu=mean.2,Sigma=Sigma)
par(mfrow=c(2,2))
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]),
col=c(rep('red',N),rep('green',N)), main="Artificial data", xlab="X1", ylab="X2")
prob.error <- function (Pw1, Pw2, Sigma, Mu1, Mu2)
{
stopifnot (Pw2+Pw1==1,Pw2>0,Pw1>0,Pw2<1,Pw1<1)
alpha <- log(Pw2/Pw1)
D <- quad.form.inv (Sigma, Mu1-Mu2)
A1 <- (alpha-D/2)/sqrt(D)
A2 <- (alpha+D/2)/sqrt(D)
Pw1*pnorm(A1)+Pw2*(1-pnorm(A2))
}
prob.error(0.5,0.5, Sigma, mean.1, mean.2)
# Numerically correct way for t(x) %*% solve(M) %*% (x)
# that is, for the quadratic form x^T M^-1 x
quad.form.inv <- function (M, x)
{
drop(crossprod(x, solve(M, x)))
}
prob.error (0.5,0.5,Sigma,mean.1,mean.2)
?crossprod
?solve
?pnorm
?drop
prob.error (0.5,0.5,Sigma,mean.1,mean.2)
d <- data.frame(c(rep(1,N),rep(2,N)), c(X.red[,1], X.green[,1]), c(X.red[,2], X.green[,2]))
colnames(d) <- c("target", "X1", "X2")
d$target <- as.factor(d$target)
summary(d)
# call to LDA
myLDA <- lda(d[c(2,3)],d[,1])
myLDA
LDAslope <- myLDA$scaling[2]/myLDA$scaling[1]
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]), col=c(rep('red',N),rep('green',N)),
main="Artificial data using LDA", xlab="X1", ylab="X2")
abline(0,LDAslope,col='black',lwd=2)
myLDA.proj <- d[,2] * myLDA$scaling[1] + d[,3] * myLDA$scaling[2]
plot(myLDA.proj, c(rep(0,N),rep(0,N)), col=c(rep('green',N),rep('red',N)),
main='Projection as seen in 1D', xlab="Discriminant", ylab="")
myLDA
myLDA$scaling
## the crabs data is also in the MASS package
data(crabs)
## look at data
?crabs
summary(crabs)
head(crabs)
Crabs.class <- factor(paste(crabs[,1],crabs[,2],sep=""))
# Now 'BF' stands now for 'Blue Female', and so on
table(Crabs.class)
## using the rest of the variables as predictors (except 'index', which is only an index)
Crabs <- crabs[,4:8]
summary(Crabs)
par(mfrow=c(1,1))
boxplot(Crabs)
hist(Crabs$FL,col='red',breaks=20,xlab="", main='Frontal Lobe Size (mm)')
hist(Crabs$RW,col='red',breaks=20,xlab="", main='Rear Width (mm)')
hist(Crabs$CL,col='red',breaks=20,xlab="", main='Carapace Length (mm)')
hist(Crabs$CW,col='red',breaks=20,xlab="", main='Carapace Width (mm)')
hist(Crabs$BD,col='red',breaks=20,xlab="", main='Body Depth (mm)')
pairs(Crabs, main="Pairs plot for the crabs", pch=21, bg=c('black', 'red', 'green', 'blue')[Crabs.class])
plot(Crabs,col=unclass(Crabs.class))
library(klaR)
partimat (x=Crabs, grouping=Crabs.class, method="lda")
install.packages("klaR")
library(klaR)
partimat (x=Crabs, grouping=Crabs.class, method="lda")
(lda.model <- lda (x=Crabs, grouping=Crabs.class))
plot(lda.model)
loadings <- as.matrix(Crabs) %*% as.matrix(lda.model$scaling)
colors.crabs <- c('blue', 'lightblue', 'orange', 'yellow')
crabs.plot <- function (myloadings)
{
plot (myloadings[,1], myloadings[,2], type="n", xlab="LD1", ylab="LD2")
text (myloadings[,1], myloadings[,2], labels=crabs$index, col=colors.crabs[unclass(Crabs.class)], cex=.55)
legend('topright', c("B-M","B-F","O-M","O-F"), fill=colors.crabs, cex=.55)
}
crabs.plot (loadings)
Crabs.new <- data.frame (New.feature = loadings, Target = Crabs.class)
summary(Crabs.new)
lda.model
library(rgl)
install.packages("rgl")
library(rgl)
# 3d scatterplot (that can be rotated)
plot3d(loadings[,1], loadings[,2], loadings[,3], "LD1", "LD2", "LD3",
size = 4, col=colors.crabs[unclass(Crabs.class)], main="Crabs Data")
library(rgl)
(lda.logmodel <- lda (x=log(Crabs), grouping=Crabs.class))
logloadings <- as.matrix(log(Crabs)) %*% as.matrix(lda.logmodel$scaling)
crabs.plot (logloadings)
(ct <- table(Truth=Crabs.class, Pred=predict(lda.logmodel, log(Crabs))$class))
# percent by class
diag(prop.table(ct, 1))
# total percent correct
sum(diag(prop.table(ct)))
lda.logmodel <- lda (x=log(Crabs), grouping=Crabs.class, prior = c(1,1,1,1)/4, CV=TRUE)
## the list 'class' gives the predicted classes
lda.logmodel$class[1:10]
# percent correct for each class
(ct <- table(Truth=Crabs.class, Pred=lda.logmodel$class))
diag(prop.table(ct, 1))
# total percent correct
sum(diag(prop.table(ct)))
lda.logmodel$posterior[1:10,]
chisq.test (lda.logmodel$class, Crabs.class)
?chisq.test
wine <- read.table("wine.data", sep=",", dec=".", header=FALSE)
dim(wine)
colnames(wine) <- c('Wine.type','Alcohol','Malic.acid','Ash','Alcalinity.of.ash','Magnesium','Total.phenols','Flavanoids','Nonflavanoid.phenols','Proanthocyanins','Color.intensity','Hue','OD280.OD315','Proline')
wine$Wine.type <- factor(wine$Wine.type, labels=c("Cultivar.1","Cultivar.2","Cultivar.3"))
summary(wine)
plot(subset(wine,select=-Wine.type),col=unclass(wine$Wine.type))
(lda.model <- lda (Wine.type ~ ., data = wine))
plot(lda.model)
wine.pred <- predict(lda.model)
plot(wine.pred$x,type="n")
text(wine.pred$x,labels=as.character(rownames(wine.pred$x)),col=as.integer(wine$Wine.type), cex=.75)
legend('bottomright', c("Cultivar 1","Cultivar 2","Cultivar 3"), lty=1, col=c('black', 'red', 'green'))
plot.mean <- function (mywine)
{
m1 <- mean(subset(wine.pred$x[,1],wine$Wine.type==mywine))
m2 <- mean(subset(wine.pred$x[,2],wine$Wine.type==mywine))
print(c(m1,m2))
points(m1,m2,pch=16,cex=1.5,col=as.integer(substr(mywine, 10, 10)))
}
plot.mean ('Cultivar.1')
plot.mean ('Cultivar.2')
plot.mean ('Cultivar.3')
table(Truth=wine$Wine.type, Pred=wine.pred$class)
lda.model.loocv <- update(lda.model, CV=TRUE)
head(lda.model.loocv$posterior)
(ct <- table(Truth=wine$Wine.type, Pred=lda.model.loocv$class))
chisq.test(ct) # indeed a very good model
qda.model <- qda (Wine.type ~ ., data = wine)
qda.model
qda.model.loocv <- qda (Wine.type ~ ., data = wine, CV=TRUE)
head(qda.model.loocv$posterior)
(ct <- table(Truth=wine$Wine.type, Pred=qda.model.loocv$class))
chisq.test(ct) # a slightly better model
set.seed(1)
(rda.model.cv <- rda (Wine.type ~ ., data = wine, fold=20, prior=1))
library(class)
s <- sqrt(1/4)
set.seed(1234)
generate <- function (M, n=100, Sigma=diag(2)*s)
{
z <- sample(1:nrow(M), n, replace=TRUE)
t(apply(M[z,],1, function(mu) mvrnorm(1,mu,Sigma)))
}
s <- sqrt(1/4)
set.seed(1234)
generate <- function (M, n=100, Sigma=diag(2)*s)
{
z <- sample(1:nrow(M), n, replace=TRUE)
t(apply(M[z,],1, function(mu) mvrnorm(1,mu,Sigma)))
}
# generate 10 means in two dimensions
M0 <- mvrnorm(10, c(1,0), diag(2))
# generate data out of M0
x0 <- generate(M0)
# repeat with M1
M1 <- mvrnorm(10, c(0,1), diag(2))
x1 <- generate(M1)
# Bind them together (by rows)
train <- rbind(x0, x1)
(N <- dim(train)[1])
# generate class labels in {0,1}
t <- c(rep(0,100), rep(1,100))
grid.size <- 100
XLIM <- range(train[,1])
grid.x <- seq(XLIM[1], XLIM[2], len=grid.size)
YLIM <- range(train[,2])
grid.y <- seq(YLIM[1], YLIM[2], len=grid.size)
test <- expand.grid(grid.x,grid.y)
dim(test)
nicecolors <- c('black','red')
visualize.1NN <- function ()
{
par(mfrow=c(1,1))
predicted <- knn (train, test, t, k=1)
# These are the predictions
plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
points(test, col=nicecolors[as.numeric(predicted)], pch=".")
contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size),
levels=c(1,2), add=TRUE, drawlabels=FALSE)
# Add training points, for reference
points(train, col=nicecolors[t+1], pch=16)
title("1-NN classification")
}
visualize.1NN ()
diag(2)
M0
generate(M0)
M0
z <- sample(1:nrow(M0), 5, replace=TRUE)
z
Sigma=diag(2)*s
Sigma
apply(M[z,],1, function(mu) mvrnorm(1,mu,Sigma))
apply(M0[z,],1, function(mu) mvrnorm(1,mu,Sigma))
z
M0
# repeat with M1
M1 <- mvrnorm(10, c(0,1), diag(2))
x1 <- generate(M1)
# Bind them together (by rows)
train <- rbind(x0, x1)
(N <- dim(train)[1])
# generate class labels in {0,1}
t <- c(rep(0,100), rep(1,100))
grid.size <- 100
XLIM <- range(train[,1])
grid.x <- seq(XLIM[1], XLIM[2], len=grid.size)
YLIM <- range(train[,2])
grid.y <- seq(YLIM[1], YLIM[2], len=grid.size)
test <- expand.grid(grid.x,grid.y)
dim(test)
nicecolors <- c('black','red')
visualize.1NN <- function ()
{
par(mfrow=c(1,1))
predicted <- knn (train, test, t, k=1)
# These are the predictions
plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
points(test, col=nicecolors[as.numeric(predicted)], pch=".")
contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size),
levels=c(1,2), add=TRUE, drawlabels=FALSE)
# Add training points, for reference
points(train, col=nicecolors[t+1], pch=16)
title("1-NN classification")
}
visualize.1NN ()
par(mfrow=c(2,3))
for (myk in c(1,3,5,7,10,round(sqrt(N))))
{
predicted <- knn(train, test, t, k=myk)
plot(train, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n")
points(test, col=nicecolors[as.numeric(predicted)], pch=".")
contour(grid.x, grid.y, matrix(as.numeric(predicted),grid.size,grid.size),
levels=c(1,2), add=TRUE, drawlabels=FALSE)
# add training points, for reference
points(train, col=nicecolors[t+1], pch=16)
title(paste(myk,"-NN classification",sep=""))
}
wine.data <- subset (wine, select = -Wine.type)
knn.preds <- rep(NA, nrow(wine.data))
for (i in 1:nrow(wine.data))
{
knn.preds[i] <- knn (wine.data[-i,], wine.data[i,], wine$Wine.type[-i], k = 3)
}
(tab <- table(Truth=wine$Wine.type, Preds=knn.preds))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
myknn.cv <- knn.cv (wine.data, wine$Wine.type, k = 3)
(tab <- table(Truth=wine$Wine.type, Preds=myknn.cv))
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
## Let's loop over k using a function
set.seed (6046)
N <- nrow(wine)
neighbours <- 1:sqrt(N)
loop.k <- function (mydata, mytargets, myneighbours)
{
errors <- matrix (nrow=length(myneighbours), ncol=2)
colnames(errors) <- c("k","LOOCV error")
for (k in myneighbours)
{
print(k)
myknn.cv <- knn.cv (mydata, mytargets, k = myneighbours[k])
# fill in number of neighbours and LOOCV error
errors[k, "k"] <- myneighbours[k]
tab <- table(Truth=mytargets, Preds=myknn.cv)
errors[k, "LOOCV error"] <- 1 - sum(tab[row(tab)==col(tab)])/sum(tab)
}
errors
}
par(mfrow=c(1,1))
plot(loop.k (wine.data, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
plot(loop.k (scale(wine.data), wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
lda.model <- lda (Wine.type ~ ., data = wine)
loadings <- as.matrix(wine.data) %*% as.matrix(lda.model$scaling)
## Let's repeat the loop over k
set.seed (6046)
plot(loop.k (loadings, wine$Wine.type, neighbours), type="l", xaxt = "n")
axis(1, neighbours)
library (e1071)
data (HouseVotes84, package="mlbench")
colnames(HouseVotes84) <-
c("Class","handicapped.infants","water.project.sharing","budget.resolution",
"physician.fee.freeze","el.salvador.aid","religious.groups.in.schools",
"anti.satellite.ban", "aid.to.nicaraguan.contras","mx.missile",
"immigration","synfuels.cutback","education.spending","superfund","crime",
"duty.free.exports","export.South.Africa")
install.packages("e1071")
library (e1071)
data (HouseVotes84, package="mlbench")
colnames(HouseVotes84) <-
c("Class","handicapped.infants","water.project.sharing","budget.resolution",
"physician.fee.freeze","el.salvador.aid","religious.groups.in.schools",
"anti.satellite.ban", "aid.to.nicaraguan.contras","mx.missile",
"immigration","synfuels.cutback","education.spending","superfund","crime",
"duty.free.exports","export.South.Africa")
summary(HouseVotes84)
set.seed(1111)
N <- nrow(HouseVotes84)
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
model <- naiveBayes(Class ~ ., data = HouseVotes84[learn,])
# we get all the probabilities (so 'eager' mode)
model
library (e1071)
data (HouseVotes84, package="mlbench")
install.packages("mlbench")
data (HouseVotes84, package="mlbench")
colnames(HouseVotes84) <-
c("Class","handicapped.infants","water.project.sharing","budget.resolution",
"physician.fee.freeze","el.salvador.aid","religious.groups.in.schools",
"anti.satellite.ban", "aid.to.nicaraguan.contras","mx.missile",
"immigration","synfuels.cutback","education.spending","superfund","crime",
"duty.free.exports","export.South.Africa")
summary(HouseVotes84)
set.seed(1111)
N <- nrow(HouseVotes84)
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
model <- naiveBayes(Class ~ ., data = HouseVotes84[learn,])
# we get all the probabilities (so 'eager' mode)
model
?naiveBayes
# predict the outcome of the first 10 Congressmen
predict(model, HouseVotes84[1:10,-1])
# same but displaying posterior probabilities
predict(model, HouseVotes84[1:10,-1], type = "raw")
# compute now the apparent error
pred <- predict(model, HouseVotes84[learn,-1])
# form and display confusion matrix & overall error
(tab <- table(Truth=HouseVotes84[learn,]$Class, Preds=pred) )
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
# compute the test (prediction) error
pred <- predict(model, newdata=HouseVotes84[-learn,-1])
# form and display confusion matrix & overall error
(tab <- table(Truth=HouseVotes84[-learn,]$Class, Preds=pred) )
1 - sum(tab[row(tab)==col(tab)])/sum(tab)
model <- naiveBayes(Class ~ ., data = HouseVotes84[learn,], laplace = 1)
library(MASS)
data (fgl)
summary(fgl)
attach(fgl)
par(mfrow=c(3,3))
my.glass.boxplot <- function (var)
{ boxplot (var ~ type, data=fgl, col = "bisque"); title(deparse(substitute(var))) }
my.glass.boxplot (RI)
my.glass.boxplot (Na)
my.glass.boxplot (Mg)
my.glass.boxplot (Al)
my.glass.boxplot (Si)
my.glass.boxplot (K)
my.glass.boxplot (Ca)
my.glass.boxplot (Ba)
my.glass.boxplot (Fe)
