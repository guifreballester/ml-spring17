set.seed(1234)
N <- 100
n1 <- 70
p <- seq(from=1e-5, to=1, by=0.01)
# this is the likelihood function (as a function of p); note it tops at 0.7
L <- choose(N,n1)*p^n1*(1-p)^(N-n1)
plot(p,L,type="l",ylab="likelihood of p",xaxt = "n")
grid(nx=10)
axis(side = 1, at = seq(0,1,by=0.1), las = 2, hadj = 0.9)
logL <- log(choose(N,n1)) + n1*log(p) + (N-n1)*log(1-p)
plot(p,logL,type="l",ylab="log-likelihood of p",xaxt = "n")
grid(nx=10)
axis(side = 1, at = seq(0,1,by=0.1), las = 2, hadj = 0.9)
N <- 500
N <- 500
x <- rnorm(n=N, mean=3, sd=2)     # generate the x_n data (note x is a vector)
beta_1 <- 0.6 ; beta_0 <- -1.5    # this is the ground truth, which is unknown
p <- 1/(1+exp( -(beta_1*x + beta_0) ))  # generate the p_n (note p is a vector)
t <- rbinom (n=N,size=1,prob=p)    # generate the targets (classes) according to p
t <- as.factor(t)                 # note t is a vector
# just for plotting, we convert t to number
plot(x,as.numeric(t)-1)
glm.res <- glm (t~x, family = binomial(link=logit))
summary(glm.res)
coef(glm.res)
glm.res$coefficients["x"]
glm.res$coefficients["(Intercept)"]
exp(glm.res$coefficients["x"])
# let us try now a "logistic plot" using predict()
M <- max(x)
m <- min(x)
abscissae <- m + (-10:210)*(M-m)/200
# with type="response" we get the predicted probability
preds <- predict (glm.res, data.frame(x=abscissae),type="response")
plot(p~x,ylim=c(0,1)) # plot previous data
lines(abscissae, preds, col="blue") # add our model, quite good!
N <- 500
x <- runif(n=N,0.1,12)            # generate the x_n (note x is a vector)
beta_1 <- 0.35 ; beta_0 <- -1     # this is the ground truth, which is unknown
l <- exp( beta_1*x+beta_0 )       # generate the lambda (note l is a vector)
t <- rpois (n=N, lambda = l)       # generate the targets t according to l
plot(x,t,xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
# Fitting of Poisson regression
mydata <- data.frame(h.week=t, dist=x)
glm.res <- glm(h.week ~ dist, family = poisson(link="log"), data = mydata)
summary(glm.res)
exp(0.343791)
(new.d <- seq(0,30,length.out=100))
fv <- predict (glm.res, data.frame(dist=new.d),se=TRUE)
plot (x,t,xlab="Distance to workplace (km)", ylab="Time wasted (h/week)")
lines (new.d,exp(fv$fit), col='green')
lines (new.d,exp(fv$fit+1.967*fv$se.fit), col='red',lty=2)
lines (new.d,exp(fv$fit-1.967*fv$se.fit), col='red',lty=2)
# logarithms (in base 2) of used doses, repeated twice (for males and for females)
(logdose <- rep(0:5,2))
# logarithms (in base 2) of used doses, repeated twice (for males and for females)
(logdose <- rep(0:5,2))
# number of dead insects at different doses, for each sex
(numdead <- c(1,4,9,13,18,20,0,2,6,10,12,16))
(sex <- factor(rep(c("M","F"),each=6)))
(budworm <- data.frame (logdose,numdead,numalive=20-numdead,sex))
# We declare the number of "trials" (the fixed 20 insects) with the 'weights' parameter
bud.logreg <- glm (numdead/20 ~ logdose + sex, family=binomial, data=budworm, weights=rep(20,length(logdose)))
# Since the number of insects is constant (20) in all trials, we can use an equivalent command, with a simpler syntax
bud.logreg <- glm(cbind(numdead,numalive) ~ logdose + sex, family=binomial, data=budworm)
# in this model, the effect of sex is significant, though less than the log of the dose
summary(bud.logreg)
# plots x axis in logarithmic scale  (because of logdose)
plot(c(1,32),c(0,1),type="n",xlab="dose",ylab="non-survival probabiblity",log="x")
# plot the data
text(2^logdose,numdead/20,labels=as.character(sex),col=as.integer(sex))
grid <- 1:32
# compute the model for males on grid, using "predict" on the values with log_2(grid)
model.on.males <- predict(bud.logreg, data.frame(logdose=log2(grid),sex="M"),type="response",se=TRUE)
# plot the model
lines(grid,model.on.males$fit,col="red")
lines (grid,model.on.males$fit+1.967*model.on.males$se.fit, col='red',lty=2)
lines (grid,model.on.males$fit-1.967*model.on.males$se.fit, col='red',lty=2)
# the same with sex = "F"
model.on.females <- predict(bud.logreg, data.frame(logdose=log2(grid),sex="F"),type="response",se=TRUE)
lines(grid,model.on.females$fit,col="black")
lines (grid,model.on.females$fit+1.967*model.on.females$se.fit, col='black',lty=2)
lines (grid,model.on.females$fit-1.967*model.on.females$se.fit, col='black',lty=2)
set.seed(3145)
Admis <- read.csv("Admissions.csv")
## view the first few rows of the data
head(Admis)
summary(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
Admis$rank <- factor(Admis$rank)
## two-way contingency table of outcome and rank,
## we want to make sure the data looks OK (no zeros or something strange)
xtabs(~admit + rank, data = Admis)
N <- nrow(Admis)
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
Admis.logreg <- glm (admit ~ gre + gpa + rank, data = Admis[learn,], family = "binomial")
summary(Admis.logreg)
Admis.logreg.step <- step(Admis.logreg)
Admis.logreg <- glm (Admis.logreg.step$formula, data = Admis[learn,], family=binomial)
summary(Admis.logreg)
## We can interpret the coefficients
exp(Admis.logreg$coefficients)
plot (Admis.logreg$linear.predictors, Admis.logreg$fitted.values, col=Admis[,"admit"])
tmp <- with(Admis, data.frame(gpa = rep(seq(from = 2, to = 4, length.out = 100), 4),
gre = mean(gre), rank = factor(rep(1:4, each = 100))))
self.test.data <- cbind(tmp, predict(Admis.logreg, newdata = tmp, type = "response", se = TRUE))
## BTW, this is a very powerful plotting library
library(ggplot2)
## plot with the predicted probabilities, with 95% confidence intervals
p <- ggplot(self.test.data, aes(x = gpa, y = fit, colour = rank))
p <- p + geom_ribbon(aes(ymin = fit - se.fit, ymax = fit + se.fit, fill = rank),
alpha = 0.25, colour = NA) + geom_line()
## view it
p
## let us first calculate the prediction error in the learn data
glfpred<-NULL
glfpred[Admis.logreg$fitted.values<0.5]<-0
glfpred[Admis.logreg$fitted.values>=0.5]<-1
(tab <- with(Admis, table(Truth=admit[learn],Pred=glfpred)))
(error.learn <- 100*(1-sum(diag(tab))/nlearn))
## do the same in the leftout data (test data)
glft <- predict(Admis.logreg, newdata=Admis[-learn,], type="response")
glfpredt <- NULL
glfpredt[glft<0.5]<-0
glfpredt[glft>=0.5]<-1
(tab <- with(Admis, table(Truth=admit[-learn],Pred=glfpredt)))
(error.test <- 100*(1-sum(diag(tab))/ntest))
table(Admis$admit)[2]/N
library(kernlab)
data(spam)
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))
spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]
george.vars <- 25:28
spam2 <- spam2[,-george.vars]
moneys.vars <- c(16,17,20,24)
spam3 <- data.frame( spam2[,-moneys.vars], spam2[,16]+spam2[,17]+spam2[,20]+spam2[,24])
colnames(spam3)[51] <- "about.money"
dim(spam3)
set.seed (4321)
N <- nrow(spam3)
learn <- sample(1:N, round(0.67*N))
nlearn <- length(learn)
ntest <- N - nlearn
## Fit a GLM in the learning data
spamM1 <- glm (type ~ ., data=spam3[learn,], family=binomial)
## Simplify it using the AIC (this may take a while, since there are many variables)
# (this takes a while)
spamM1.AIC <- step (spamM1)
## Fit a GLM in the learning data
spamM1 <- glm (type ~ ., data=spam3[learn,], family=binomial)
coef(spamM1)
# 'P' is a parameter; whenever our filter assigns spam with probability at least P then we predict spam
spam.accs <- function (P=0.5)
{
## Compute accuracy in learning data
spamM1.AICpred <- NULL
spamM1.AICpred[spamM1.AIC$fitted.values<P] <- 0
spamM1.AICpred[spamM1.AIC$fitted.values>=P] <- 1
spamM1.AICpred <- factor(spamM1.AICpred, labels=c("nonspam","spam"))
print(M1.TRtable <- table(Truth=spam3[learn,]$type,Pred=spamM1.AICpred))
print(100*(1-sum(diag(M1.TRtable))/nlearn))
## Compute accuracy in test data
gl1t <- predict(spamM1.AIC, newdata=spam3[-learn,],type="response")
gl1predt <- NULL
gl1predt[gl1t<P] <- 0
gl1predt[gl1t>=P] <- 1
gl1predt <- factor(gl1predt, labels=c("nonspam","spam"))
print(M1.TEtable <- table(Truth=spam3[-learn,]$type,Pred=gl1predt))
print(100*(1-sum(diag(M1.TEtable))/ntest))
}
spam.accs()
## Simplify it using the AIC (this may take a while, since there are many variables)
# (this takes a while)
spamM1.AIC <- step (spamM1)
# CK values
(CK <- seq(20,460,by=40))
# number of patients suffering a heart attack
(num.heart <- c(2,13,30,30,21,19,18,13,19,15,7,8))
# number of patients not suffering a heart attack
(num.non.heart <- c(88,26,8,5,0,1,1,1,1,0,0,0))
(heart.attack <- data.frame (CK,num.heart,num.non.heart))
p <- heart.attack$num.heart/(heart.attack$num.heart+heart.attack$num.non.heart)
plot (heart.attack$CK, p, xlab="CK level", ylab="Proportion of heart attack")
mod.1 <- glm(cbind(num.heart,num.non.heart) ~ CK, family=binomial, data=heart.attack)
summary(mod.1)
## This asks for a residuals versus fitted values plot
plot(mod.1, which=1)
1-pchisq(36.929,10)
mod.2 <- glm(cbind(num.heart,num.non.heart) ~ CK + I(CK^2) + I(CK^3), family=binomial, data=heart.attack)
summary(mod.2)
1-pchisq(4.2525,8)
plot(mod.2, which=1)
cks <- seq(10,500,length.out=100)
has <- predict (mod.2, data.frame(CK=cks),se=TRUE, type="response")
plot (heart.attack$CK, p, xlab="CK level", ylab="Proportion of heart attack")
lines(cks,has$fit,col="red")
lines (cks,has$fit+1.967*has$se.fit, col='red',lty=2)
lines (cks,has$fit-1.967*has$se.fit, col='red',lty=2)
